{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step instructions to download HEST-1k \n",
    "\n",
    "This tutorial will guide you to:\n",
    "\n",
    "- Download HEST-1k in its entirety (scanpy, whole-slide images, patches, nuclear segmentation, alignment preview)\n",
    "- Download some samples of HEST-1k \n",
    "- Download samples with some attributes (e.g., all breast cancer cases) \n",
    "- Inspect freshly downloaded samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Setting Up HuggingFace Account and Token\n",
    "\n",
    "### 1. Create an Account on HuggingFace\n",
    "Follow the instructions provided on the [HuggingFace sign-up page](https://huggingface.co/join).\n",
    "\n",
    "### 2. Accept terms of use of HEST\n",
    "\n",
    "1. Go to [HEST HuggingFace page](https://huggingface.co/datasets/MahmoodLab/hest)\n",
    "2. Request access (access will be automatically granted)\n",
    "3. At this stage, you can already manually inspect the data by navigating in the `Files and version`\n",
    "\n",
    "### 3. Create a Hugging Face Token\n",
    "\n",
    "1. **Go to Settings:** Navigate to your profile settings by clicking on your profile picture in the top right corner and selecting `Settings` from the dropdown menu.\n",
    "\n",
    "2. **Access Tokens:** In the settings menu, find and click on `Access tokens`.\n",
    "\n",
    "3. **Create New Token:**\n",
    "   - Click on `New token`.\n",
    "   - Set the token name (e.g., `hest`).\n",
    "   - Set the access level to `Write`.\n",
    "   - Click on `Create`.\n",
    "\n",
    "4. **Copy Token:** After the token is created, copy it to your clipboard. You will need this token for authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logging\n",
    "\n",
    "Install the python library `datasets` and run cell below. If successful, you should see:\n",
    "\n",
    "```\n",
    "Your token has been saved to /home/usr/.cache/huggingface/token\n",
    "Login successful\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YOUR HUGGING FACE TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download HEST-1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "local_dir='../hest_data' # hest will be dowloaded to this folder\n",
    "\n",
    "# Note that the full dataset is around 1TB of data\n",
    "dataset = datasets.load_dataset(\n",
    "    'MahmoodLab/hest', \n",
    "    cache_dir=local_dir,\n",
    "    patterns='*'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download HEST-1k based on sample IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "local_dir='../hest_data' # hest will be dowloaded to this folder\n",
    "\n",
    "ids_to_query = ['TENX95', 'TENX99'] # list of ids to query\n",
    "\n",
    "list_patterns = [f\"*{id}[_.]**\" for id in ids_to_query]\n",
    "dataset = datasets.load_dataset(\n",
    "    'MahmoodLab/hest', \n",
    "    cache_dir=local_dir,\n",
    "    patterns=list_patterns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download HEST-1k based on metadata keys (e.g., organ, technology, oncotree code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "local_dir='../hest_data' # hest will be dowloaded to this folder\n",
    "\n",
    "meta_df = pd.read_csv(\"hf://datasets/MahmoodLab/hest/HEST_v1_0_2.csv\")\n",
    "\n",
    "# Filter the dataframe by organ, oncotree code...\n",
    "meta_df = meta_df[meta_df['oncotree_code'] == 'IDC']\n",
    "meta_df = meta_df[meta_df['organ'] == 'Breast']\n",
    "\n",
    "ids_to_query = meta_df['id'].values\n",
    "\n",
    "list_patterns = [f\"*{id}[_.]**\" for id in ids_to_query]\n",
    "dataset = datasets.load_dataset(\n",
    "    'MahmoodLab/hest', \n",
    "    cache_dir=local_dir,\n",
    "    patterns=list_patterns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect freshly downloaded samples\n",
    "\n",
    "For each sample, we provide:\n",
    "\n",
    "- **wsis/**: H&E-stained whole slide images in pyramidal Generic TIFF (or pyramidal Generic BigTIFF if >4.1GB)\n",
    "- **st/**: Spatial transcriptomics expressions in a scanpy .h5ad object\n",
    "- **metadata/**: Metadata\n",
    "- **spatial_plots/**: Overlay of the WSI with the st spots\n",
    "- **thumbnails/**: Downscaled version of the WSI\n",
    "- **tissue_seg/**: Tissue segmentation masks:\n",
    "    - `{id}_mask.jpg`: Downscaled or full resolution greyscale tissue mask\n",
    "    - `{id}_mask.pkl`: Tissue/holes contours in a pickle file\n",
    "    - `{id}_vis.jpg`: Visualization of the tissue mask on the downscaled WSI\n",
    "- **pixel_size_vis/**: Visualization of the pixel size\n",
    "- **patches/**: 256x256 H&E patches (0.5Âµm/px) extracted around ST spots in a .h5 object optimized for deep-learning. Each patch is matched to the corresponding ST profile (see **st/**) with a barcode.\n",
    "- **patches_vis/**: Visualization of the mask and patches on a downscaled WSI.\n",
    "- **transcripts/**: individual transcripts aligned to H&E for xenium samples; read with pandas.read_parquet; aligned coordinates in pixel are in columns `['he_x', 'he_y']`\n",
    "- **cellvit_seg/**: Cellvit nuclei segmentation\n",
    "- **xenium_seg**: xenium segmentation on DAPI and aligned to H&E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load hest...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: ../hest_data/st/TENX96.h5ad\nHave you downloaded the dataset? (https://huggingface.co/datasets/MahmoodLab/hest)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload hest...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Iterate through a subset of hest\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m st \u001b[38;5;129;01min\u001b[39;00m iter_hest(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../hest_data\u001b[39m\u001b[38;5;124m'\u001b[39m, id_list\u001b[38;5;241m=\u001b[39mid_list):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(st)\n",
      "File \u001b[0;32m~/Research/Project/HEST/src/hest/HESTData.py:1127\u001b[0m, in \u001b[0;36mHESTIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m HESTData:\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1127\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43m_read_st\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Research/Project/HEST/src/hest/HESTData.py:1157\u001b[0m, in \u001b[0;36m_read_st\u001b[0;34m(hest_dir, st_filename, load_transcripts)\u001b[0m\n\u001b[1;32m   1155\u001b[0m masks_path_pkl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m masks_path_jpg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m \u001b[43mverify_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43madata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mHave you downloaded the dataset? (https://huggingface.co/datasets/MahmoodLab/hest)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hest_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtissue_seg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m   1161\u001b[0m     masks_path_pkl \u001b[38;5;241m=\u001b[39m find_first_file_endswith(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hest_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtissue_seg\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mask.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Research/Project/HEST/src/hest/utils.py:105\u001b[0m, in \u001b[0;36mverify_paths\u001b[0;34m(paths, suffix)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: ../hest_data/st/TENX96.h5ad\nHave you downloaded the dataset? (https://huggingface.co/datasets/MahmoodLab/hest)"
     ]
    }
   ],
   "source": [
    "from hest import iter_hest\n",
    "import pandas as pd\n",
    "\n",
    "# Ex: inspect all the Invasive Lobular Carcinoma samples (ILC)\n",
    "meta_df = pd.read_csv('../assets/HEST_v1_1_0.csv')\n",
    "\n",
    "id_list = meta_df[meta_df['oncotree_code'] == 'ILC']['id'].values\n",
    "\n",
    "print('load hest...')\n",
    "# Iterate through a subset of hest\n",
    "for st in iter_hest('../hest_data', id_list=id_list):\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = iter_hest('/mnt/DATA16T/hest_data', id_list=['TENX111'], load_transcripts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whytin/miniconda3/envs/hest/lib/python3.9/site-packages/hestcore/wsi.py:27: UserWarning: CuImage is not available. Ensure you have a GPU and cucim installed to use GPU acceleration.\n",
      "  warnings.warn(\"CuImage is not available. Ensure you have a GPU and cucim installed to use GPU acceleration.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hest.HESTData.XeniumHESTData object at 0x771d8e268cd0>\n",
      "        'pixel_size' is 0.2738047666871285\n",
      "        'wsi' is <width=26909, height=43090, backend=OpenSlideWSI>\n",
      "        'shapes': [name: cellvit, coord-system: he, <not loaded>, name: xenium_cell, coord-system: he, <not loaded>, name: xenium_nucleus, coord-system: he, <not loaded>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whytin/miniconda3/envs/hest/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py:432: RuntimeWarning: invalid value encountered in divide\n",
      "  return values / sums[:, None]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
